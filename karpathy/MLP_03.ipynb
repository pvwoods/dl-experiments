{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "867ed7f4-162e-4d51-baba-06b0673506e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a124671-41cb-4006-8d3c-5de7544e20f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('./names.txt').read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cc909a7-4e9b-4774-867a-50fc2accbccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words)))) #lol\n",
    "token_lookup = {c: i+1 for i, c in enumerate(chars)}\n",
    "token_lookup['.'] = 0\n",
    "char_lookup = {i:c for c, i in token_lookup.items()}\n",
    "TOTAL_TOKENS = len(char_lookup.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "416881c5-3bb0-40ae-8277-64754437084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "BLOCK_SIZE = 3 # context size to give the model in order to predict the next character\n",
    "\n",
    "def build_dataset(corpus, block_size, codebook, padding_char=\".\"):\n",
    "    X, Y = [], []\n",
    "    for word in corpus:\n",
    "        start_padding = padding_char * block_size\n",
    "        padded_word = f\"{start_padding}{word}.\"\n",
    "        tokenized_word = [codebook[c] for c in padded_word]\n",
    "        for i in range(len(tokenized_word)-block_size):\n",
    "            X.append(tokenized_word[i:i+block_size])\n",
    "            Y.append(tokenized_word[i+block_size])\n",
    "        \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(len(words)*0.8)\n",
    "n2 = int(len(words)*0.9)\n",
    "\n",
    "x_train, y_train = build_dataset(words[:n1], BLOCK_SIZE, token_lookup)\n",
    "x_valid, y_valid = build_dataset(words[n1:n2], BLOCK_SIZE, token_lookup)\n",
    "x_test, y_test = build_dataset(words[n2:], BLOCK_SIZE, token_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91a02a86-7506-4b4c-a4d0-29fca6f53ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \n",
    "    def __init__(self, fan_in, fan_out, use_bias=True):\n",
    "        \n",
    "        self.weights = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out) if use_bias else None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = x * self.weights\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def paramaters(self):\n",
    "        return [self.weights] + ([] if self.bias is None else [self.bias])\n",
    "    \n",
    "class BatchNorm1d:\n",
    "    \n",
    "    def __init__(self, dims, momentum=0.001, epsilon=1e-5):\n",
    "        \n",
    "        self.dims = dims\n",
    "        self.running_mean = torch.zeros(self.dims)\n",
    "        self.running_var = torch.ones(self.dims)\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = torch.ones(self.dims)\n",
    "        self.beta = torch.zeros(self.dims)\n",
    "        self.training = True\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            cur_mean = x.mean(0, keepdims=True)\n",
    "            cur_var = x.var(0, keepdims=True, unbiased=True)\n",
    "        else:\n",
    "            cur_mean = self.running_mean\n",
    "            cur_var = self.running_std\n",
    "        \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                self.running_mean = ((1-self.momentum) * self.running_mean) + (self.momentum * cur_mean)\n",
    "                self.running_var = ((1-self.momentum) * self.running_var) + (self.momentum * cur_var)\n",
    "        \n",
    "        self.out = (x - cur_mean) / torch.sqrt(cur_var + self.epsilon)\n",
    "        self.out = (self.gamma * self.out) + self.beta\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def paramaters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "class Tanh:\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "    def paramaters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "80f00061-a1d2-45d7-aedc-846a62d6be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    \n",
    "    def __init__(self, embedding_dims, vocab_size, hidden_states, device='cpu'):\n",
    "        \n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.hidden_states = hidden_states\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding_layer = torch.randn((self.vocab_size, self.embedding_dims))\n",
    "        \n",
    "        self.layers = [\n",
    "            Linear(self.embedding_dims, self.hidden_states, False),\n",
    "            BatchNorm1d(self.hidden_states),\n",
    "            Tanh(),\n",
    "            Linear(self.hidden_states, self.vocab_size)\n",
    "        ]\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            for param in layer.paramaters():\n",
    "                param.requires_grad = True\n",
    "                param.to(device)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.out = self.embedding_layer[x]\n",
    "        for layer in self.layers:\n",
    "            self.out = layer(self.out)\n",
    "        return self.out\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c5b4286c-7c22-47d7-ac13-7c3d6b3c5843",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(embedding_dims=10, vocab_size=TOTAL_TOKENS, hidden_states=200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
