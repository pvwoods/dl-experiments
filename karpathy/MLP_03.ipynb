{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "867ed7f4-162e-4d51-baba-06b0673506e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a124671-41cb-4006-8d3c-5de7544e20f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('./names.txt').read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3cc909a7-4e9b-4774-867a-50fc2accbccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words)))) #lol\n",
    "token_lookup = {c: i+1 for i, c in enumerate(chars)}\n",
    "token_lookup['.'] = 0\n",
    "char_lookup = {i:c for c, i in token_lookup.items()}\n",
    "TOTAL_TOKENS = len(char_lookup.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "416881c5-3bb0-40ae-8277-64754437084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "BLOCK_SIZE = 3 # context size to give the model in order to predict the next character\n",
    "\n",
    "def build_dataset(corpus, block_size, codebook, padding_char=\".\"):\n",
    "    X, Y = [], []\n",
    "    for word in corpus:\n",
    "        start_padding = padding_char * block_size\n",
    "        padded_word = f\"{start_padding}{word}.\"\n",
    "        tokenized_word = [codebook[c] for c in padded_word]\n",
    "        for i in range(len(tokenized_word)-block_size):\n",
    "            X.append(tokenized_word[i:i+block_size])\n",
    "            Y.append(tokenized_word[i+block_size])\n",
    "        \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(len(words)*0.8)\n",
    "n2 = int(len(words)*0.9)\n",
    "\n",
    "x_train, y_train = build_dataset(words[:n1], BLOCK_SIZE, token_lookup)\n",
    "x_valid, y_valid = build_dataset(words[n1:n2], BLOCK_SIZE, token_lookup)\n",
    "x_test, y_test = build_dataset(words[n2:], BLOCK_SIZE, token_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "91a02a86-7506-4b4c-a4d0-29fca6f53ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    \n",
    "    def __init__(self, fan_in, fan_out, use_bias=True):\n",
    "        \n",
    "        self.weights = torch.randn((fan_in, fan_out)) / fan_in**0.5\n",
    "        self.bias = torch.zeros(fan_out) if use_bias else None\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        self.out = x @ self.weights\n",
    "        if self.bias is not None:\n",
    "            self.out += self.bias\n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.weights] + ([] if self.bias is None else [self.bias])\n",
    "    \n",
    "class BatchNorm1d:\n",
    "    \n",
    "    def __init__(self, dims, momentum=0.001, epsilon=1e-5):\n",
    "        \n",
    "        self.dims = dims\n",
    "        # buffers\n",
    "        self.running_mean = torch.zeros(self.dims)\n",
    "        self.running_var = torch.ones(self.dims)\n",
    "        \n",
    "        # parameters\n",
    "        self.gamma = torch.ones(self.dims)\n",
    "        self.beta = torch.zeros(self.dims)\n",
    "        \n",
    "        # constants\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        self.training = True\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        if self.training:\n",
    "            cur_mean = x.mean(0, keepdims=True)\n",
    "            cur_var = x.var(0, keepdims=True, unbiased=True)\n",
    "        else:\n",
    "            cur_mean = self.running_mean\n",
    "            cur_var = self.running_var\n",
    "        \n",
    "        if self.training:\n",
    "            with torch.no_grad():\n",
    "                # calculate exponential moving average\n",
    "                self.running_mean = ((1-self.momentum) * self.running_mean) + (self.momentum * cur_mean)\n",
    "                self.running_var = ((1-self.momentum) * self.running_var) + (self.momentum * cur_var)\n",
    "        \n",
    "        self.out = (x - cur_mean) / torch.sqrt(cur_var + self.epsilon)\n",
    "        self.out = (self.gamma * self.out) + self.beta\n",
    "        \n",
    "        return self.out\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [self.gamma, self.beta]\n",
    "    \n",
    "class Tanh:\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return torch.tanh(x)\n",
    "    \n",
    "    def parameters(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "80f00061-a1d2-45d7-aedc-846a62d6be49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block:\n",
    "    \n",
    "    def __init__(self, dims_in, dims_out):\n",
    "        \n",
    "        self.training = True\n",
    "        \n",
    "        self.layers = [\n",
    "            Linear(dims_in, dims_out, False),\n",
    "            BatchNorm1d(dims_out),\n",
    "            Tanh()\n",
    "        ]\n",
    "        \n",
    "        for layer in self.layers[:-1]:\n",
    "            if isinstance(layer, Linear):\n",
    "                layer.weights *= 5/3 # tanh gain\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.out = x\n",
    "        for layer in self.layers:\n",
    "            self.out = layer(self.out)\n",
    "        return self.out\n",
    "    \n",
    "    def train(self, training=True):\n",
    "        self.training = training\n",
    "        for layer in self.layers:\n",
    "            layer.training = training\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [p for layer in self.layers for p in layer.parameters()]\n",
    "        \n",
    "\n",
    "class Model:\n",
    "    \n",
    "    def __init__(self, embedding_dims, vocab_size, hidden_states, block_size, device='cpu'):\n",
    "        \n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.hidden_states = hidden_states\n",
    "        self.vocab_size = vocab_size\n",
    "        self.block_size = block_size\n",
    "        \n",
    "        self.embedding_layer = torch.randn((self.vocab_size, self.embedding_dims))\n",
    "        \n",
    "        self.layers = [\n",
    "            Block(self.embedding_dims * self.block_size, self.hidden_states),\n",
    "            Block(self.hidden_states, self.hidden_states),\n",
    "            Block(self.hidden_states, self.hidden_states),\n",
    "            Block(self.hidden_states, self.hidden_states),\n",
    "            Block(self.hidden_states, self.hidden_states),\n",
    "            # logits\n",
    "            Linear(self.hidden_states, self.vocab_size)\n",
    "        ]\n",
    "        \n",
    "        # scaling for initialization\n",
    "        self.layers[-1].weights *= 0.1 # last layer scaled to be less confident\n",
    "        \n",
    "        \n",
    "        # update parameters for gradient\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = True\n",
    "            param.to(device)\n",
    "        \n",
    "    def parameters(self):\n",
    "        return [self.embedding_layer] + [p for layer in self.layers for p in layer.parameters()]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for param in self.parameters():\n",
    "            param.grad = None\n",
    "            \n",
    "    def apply_gradients(self, lr):\n",
    "        for param in self.parameters():\n",
    "            param.data += -lr * param.grad\n",
    "    \n",
    "    def train(self, training=True):\n",
    "        self.training = training\n",
    "        for layer in self.layers:\n",
    "            if isinstance(layer, Block):\n",
    "                layer.train(training)\n",
    "            else:\n",
    "                layer.training = training\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        self.out = self.embedding_layer[x].view(-1, self.embedding_dims * self.block_size)\n",
    "        for layer in self.layers:\n",
    "            self.out = layer(self.out)\n",
    "        return self.out\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c5b4286c-7c22-47d7-ac13-7c3d6b3c5843",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 3.290371894836426\n",
      "step 2000: 2.4756479263305664\n",
      "step 4000: 2.069335460662842\n",
      "step 6000: 2.2764081954956055\n",
      "step 8000: 2.2760443687438965\n",
      "step 10000: 2.1596033573150635\n",
      "step 12000: 1.8181756734848022\n",
      "step 14000: 1.678464651107788\n",
      "step 16000: 2.067352294921875\n",
      "step 18000: 2.169252634048462\n",
      "step 20000: 2.0569591522216797\n",
      "step 22000: 2.5262765884399414\n",
      "step 24000: 2.372263193130493\n"
     ]
    }
   ],
   "source": [
    "model = Model(embedding_dims=10, vocab_size=TOTAL_TOKENS, hidden_states=200, block_size=3)\n",
    "\n",
    "steps = 25000\n",
    "print_steps = 2000\n",
    "BATCH_SIZE = 256\n",
    "lrs = [1e-1, 1e-2]\n",
    "tlr = math.ceil(steps / len(lrs))\n",
    "\n",
    "for t in range(steps):\n",
    "    lr = lrs[int(t / tlr)]\n",
    "    batch_ix = torch.randint(0, x_train.shape[0], (32,))\n",
    "    logits = model(x_train[batch_ix])\n",
    "    \n",
    "    loss = F.cross_entropy(logits, y_train[batch_ix])\n",
    "    \n",
    "    if t % print_steps == 0:\n",
    "        print(f'step {t}: {loss.item()}')\n",
    "\n",
    "    model.zero_grad()\n",
    "    loss.backward()\n",
    "    model.apply_gradients(lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "d262b8cf-bd43-4204-ba29-57aa6e9a21e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.0840), tensor(2.1222))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(False)\n",
    "# clculate training and validation loss\n",
    "@torch.no_grad()\n",
    "def calc_loss(x_target, y_target):\n",
    "    logits = model(x_target)\n",
    "    loss = F.cross_entropy(logits, y_target)\n",
    "    return loss\n",
    "\n",
    "calc_loss(x_train, y_train), calc_loss(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "43eb0b58-ce2c-4335-8262-a90572c7b41e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dan\n",
      "ari\n",
      "nata\n",
      "keysen\n",
      "kaeta\n",
      "ali\n",
      "leoure\n",
      "chr\n",
      "kaleah\n",
      "remeha\n",
      "luquint\n",
      "zaniyana\n",
      "via\n",
      "charah\n",
      "mike\n",
      "scarrikuna\n",
      "jalynn\n",
      "moh\n",
      "jake\n",
      "merleelly\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "@torch.no_grad()\n",
    "def sample():\n",
    "    tokens = [0] * BLOCK_SIZE\n",
    "    while True:\n",
    "        cur_tokens = torch.tensor(tokens[-BLOCK_SIZE:])\n",
    "        logits = model(cur_tokens)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        tokens.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    return \"\".join([char_lookup[t] for t in tokens][BLOCK_SIZE:-1])\n",
    "\n",
    "for i in range(20):\n",
    "    print(sample())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
