{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29c7bf3a-7124-40e3-b48e-c3e780bd11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "817553a6-85c2-4401-a4c5-c6d4d958257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('./names.txt').read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a54f7e45-7d68-468a-878c-3feca2f17f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words)))) #lol\n",
    "token_lookup = {c: i+1 for i, c in enumerate(chars)}\n",
    "token_lookup['.'] = 0\n",
    "char_lookup = {i:c for c, i in token_lookup.items()}\n",
    "TOTAL_TOKENS = len(char_lookup.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86a86479-3a32-434e-9c94-930dc804d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "BLOCK_SIZE = 3 # context size to give the model in order to predict the next character\n",
    "\n",
    "def build_dataset(corpus, block_size, codebook, padding_char=\".\"):\n",
    "    X, Y = [], []\n",
    "    for word in corpus:\n",
    "        start_padding = padding_char * block_size\n",
    "        padded_word = f\"{start_padding}{word}.\"\n",
    "        tokenized_word = [codebook[c] for c in padded_word]\n",
    "        for i in range(len(tokenized_word)-block_size):\n",
    "            X.append(tokenized_word[i:i+block_size])\n",
    "            Y.append(tokenized_word[i+block_size])\n",
    "        \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(len(words)*0.8)\n",
    "n2 = int(len(words)*0.9)\n",
    "\n",
    "x_train, y_train = build_dataset(words[:n1], BLOCK_SIZE, token_lookup)\n",
    "x_valid, y_valid = build_dataset(words[n1:n2], BLOCK_SIZE, token_lookup)\n",
    "x_test, y_test = build_dataset(words[n2:], BLOCK_SIZE, token_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "1d82c534-cb15-4095-937a-545c4e29bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_NEURONS = 200\n",
    "EMBEDDING_DIMS = 10 # embed 27 characters into N dimensional space\n",
    "\n",
    "C = torch.randn((TOTAL_TOKENS, EMBEDDING_DIMS))\n",
    "W1 = torch.randn((EMBEDDING_DIMS * BLOCK_SIZE, TOTAL_NEURONS)) * ((5/3)/(EMBEDDING_DIMS * BLOCK_SIZE)**0.5) # kaiming init\n",
    "b1 = torch.randn((TOTAL_NEURONS,)) * 0.01\n",
    "W2 = torch.randn(TOTAL_NEURONS, TOTAL_TOKENS) * 0.01\n",
    "b2 = torch.randn((TOTAL_TOKENS,)) * 0.01\n",
    "\n",
    "bn_gain = torch.ones((1, TOTAL_NEURONS))\n",
    "bn_bias = torch.zeros((1, TOTAL_NEURONS))\n",
    "bn_epsilon = torch.tensor([1e-5])\n",
    "bn_mean_running = torch.zeros((1, TOTAL_NEURONS))\n",
    "bn_std_running = torch.zeros((1, TOTAL_NEURONS))\n",
    "bn_momentum = 0.001\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bn_gain, bn_bias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f539fdf3-7010-4316-a88a-b30721128f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 3.2858150005340576\n",
      "step 5000: 2.1013684272766113\n",
      "step 10000: 2.2706680297851562\n",
      "step 15000: 2.6932806968688965\n",
      "step 20000: 2.216247081756592\n",
      "step 25000: 2.454237699508667\n",
      "step 30000: 2.223783016204834\n",
      "step 35000: 1.8723540306091309\n",
      "step 40000: 2.0456132888793945\n",
      "step 45000: 2.3392839431762695\n"
     ]
    }
   ],
   "source": [
    "steps = 50000\n",
    "print_steps = 5000\n",
    "BATCH_SIZE = 256\n",
    "lrs = [1e-1, 1e-2]\n",
    "tlr = math.ceil(steps / len(lrs))\n",
    "\n",
    "for t in range(steps):\n",
    "    lr = lrs[int(t / tlr)]\n",
    "    \n",
    "    # forward pass\n",
    "    batch_ix = torch.randint(0, x_train.shape[0], (32,))\n",
    "    embeddings = C[x_train[batch_ix]]\n",
    "    cat_embeddings = embeddings.view(-1, EMBEDDING_DIMS * BLOCK_SIZE)\n",
    "    preact = cat_embeddings @ W1 + b1\n",
    "\n",
    "    # batch norm layer\n",
    "    cur_bn_mean = preact.mean(dim=0, keepdims=True)\n",
    "    cur_bn_std = preact.std(dim=0, keepdims=True) + bn_epsilon\n",
    "    preact = (preact - cur_bn_mean) / cur_bn_std\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        bn_mean_running = ((1 - bn_momentum) * bn_mean_running) + (bn_momentum * cur_bn_mean)\n",
    "        bn_std_running = ((1 - bn_momentum) * bn_std_running) + (bn_momentum * cur_bn_std)\n",
    "    \n",
    "    preact = bn_gain * preact + bn_bias\n",
    "    \n",
    "    # non linearity\n",
    "    hidden_states_01 = torch.tanh(preact)\n",
    "    \n",
    "    # second layer to logits\n",
    "    logits = hidden_states_01 @ W2 + b2\n",
    "    \n",
    "    # loss\n",
    "    loss = F.cross_entropy(logits, y_train[batch_ix])\n",
    "    \n",
    "    if t % print_steps == 0:\n",
    "        print(f'step {t}: {loss.item()}')\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "1d8437ba-5a8f-46d5-b3f8-07930537c41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.1261), tensor(2.1555))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clculate training and validation loss\n",
    "@torch.no_grad()\n",
    "def calc_loss(x_target, y_target):\n",
    "    emb = C[x_target].view(-1, EMBEDDING_DIMS * BLOCK_SIZE)\n",
    "    h = emb @ W1 + b1\n",
    "    h = (h - bn_mean_running) / bn_std_running\n",
    "    h = torch.tanh(bn_gain * h + bn_bias)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y_target)\n",
    "    return loss\n",
    "\n",
    "calc_loss(x_train, y_train), calc_loss(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "256b36b9-8c2b-4fbf-927e-8f453f69ce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "letsuhandi\n",
      "hazion\n",
      "corixkendanae\n",
      "slakyn\n",
      "mador\n",
      "kaveonte\n",
      "amay\n",
      "aryoni\n",
      "raf\n",
      "andrick\n",
      "kos\n",
      "gion\n",
      "dakjarisho\n",
      "roselaniquancoin\n",
      "ameybeexaney\n",
      "zyn\n",
      "hoduly\n",
      "aun\n",
      "kalyleynosisyq\n",
      "ado\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "@torch.no_grad()\n",
    "def sample():\n",
    "    tokens = [0] * BLOCK_SIZE\n",
    "    while True:\n",
    "        emb = C[torch.tensor(tokens[-BLOCK_SIZE:])].view(-1, EMBEDDING_DIMS * BLOCK_SIZE)\n",
    "        h = emb @ W1 + b1\n",
    "        h = (h - bn_mean_running) / bn_std_running\n",
    "        h = torch.tanh(bn_gain * h + bn_bias)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        tokens.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    return \"\".join([char_lookup[t] for t in tokens][BLOCK_SIZE:-1])\n",
    "\n",
    "for i in range(20):\n",
    "    print(sample())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
