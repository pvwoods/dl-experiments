{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "29c7bf3a-7124-40e3-b48e-c3e780bd11b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import math\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "817553a6-85c2-4401-a4c5-c6d4d958257f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('./names.txt').read().splitlines()\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a54f7e45-7d68-468a-878c-3feca2f17f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = sorted(list(set(''.join(words)))) #lol\n",
    "token_lookup = {c: i+1 for i, c in enumerate(chars)}\n",
    "token_lookup['.'] = 0\n",
    "char_lookup = {i:c for c, i in token_lookup.items()}\n",
    "TOTAL_TOKENS = len(char_lookup.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "86a86479-3a32-434e-9c94-930dc804d209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the dataset\n",
    "\n",
    "BLOCK_SIZE = 3 # context size to give the model in order to predict the next character\n",
    "\n",
    "def build_dataset(corpus, block_size, codebook, padding_char=\".\"):\n",
    "    X, Y = [], []\n",
    "    for word in corpus:\n",
    "        start_padding = padding_char * block_size\n",
    "        padded_word = f\"{start_padding}{word}.\"\n",
    "        tokenized_word = [codebook[c] for c in padded_word]\n",
    "        for i in range(len(tokenized_word)-block_size):\n",
    "            X.append(tokenized_word[i:i+block_size])\n",
    "            Y.append(tokenized_word[i+block_size])\n",
    "        \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(len(words)*0.8)\n",
    "n2 = int(len(words)*0.9)\n",
    "\n",
    "x_train, y_train = build_dataset(words[:n1], BLOCK_SIZE, token_lookup)\n",
    "x_valid, y_valid = build_dataset(words[n1:n2], BLOCK_SIZE, token_lookup)\n",
    "x_test, y_test = build_dataset(words[n2:], BLOCK_SIZE, token_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "1d82c534-cb15-4095-937a-545c4e29bbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_NEURONS = 200\n",
    "EMBEDDING_DIMS = 10 # embed 27 characters into N dimensional space\n",
    "\n",
    "C = torch.randn((TOTAL_TOKENS, EMBEDDING_DIMS))\n",
    "W1 = torch.randn((EMBEDDING_DIMS * BLOCK_SIZE, TOTAL_NEURONS)) * ((5/3)/(EMBEDDING_DIMS * BLOCK_SIZE)**0.5) # kaiming init\n",
    "b1 = torch.randn((TOTAL_NEURONS,)) * 0.01\n",
    "W2 = torch.randn(TOTAL_NEURONS, TOTAL_TOKENS) * 0.01\n",
    "b2 = torch.randn((TOTAL_TOKENS,)) * 0.01\n",
    "\n",
    "bn_gain = torch.ones((1, TOTAL_NEURONS))\n",
    "bn_bias = torch.zeros((1, TOTAL_NEURONS))\n",
    "\n",
    "parameters = [C, W1, b1, W2, b2, bn_gain, bn_bias]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f539fdf3-7010-4316-a88a-b30721128f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0: 2.5478084087371826\n",
      "step 5000: 2.377833127975464\n",
      "step 10000: 2.454554319381714\n",
      "step 15000: 2.3809127807617188\n",
      "step 20000: 2.3064322471618652\n",
      "step 25000: 2.449836492538452\n",
      "step 30000: 2.2298200130462646\n",
      "step 35000: 2.4332175254821777\n",
      "step 40000: 1.733128547668457\n",
      "step 45000: 2.1831178665161133\n"
     ]
    }
   ],
   "source": [
    "steps = 50000\n",
    "print_steps = 5000\n",
    "BATCH_SIZE = 256\n",
    "lrs = [1e-1, 1e-2]\n",
    "tlr = math.ceil(steps / len(lrs))\n",
    "\n",
    "for t in range(steps):\n",
    "    lr = lrs[int(t / tlr)]\n",
    "    # forward pass\n",
    "    batch_ix = torch.randint(0, x_train.shape[0], (32,))\n",
    "    embeddings = C[x_train[batch_ix]]\n",
    "    cat_embeddings = embeddings.view(-1, EMBEDDING_DIMS * BLOCK_SIZE)\n",
    "    preact = cat_embeddings @ W1 + b1\n",
    "    # batch norm\n",
    "    preact = (preact - preact.mean(dim=0, keepdims=True)) / preact.std(dim=0, keepdims=True)\n",
    "    preact = bn_gain * preact + bn_bias\n",
    "    hidden_states_01 = torch.tanh(preact)\n",
    "    logits = hidden_states_01 @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y_train[batch_ix])\n",
    "    if t % print_steps == 0:\n",
    "        print(f'step {t}: {loss.item()}')\n",
    "    #backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None\n",
    "    loss.backward()\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1d8437ba-5a8f-46d5-b3f8-07930537c41d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(2.1053), tensor(2.1397))"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# clculate training and validation loss\n",
    "@torch.no_grad()\n",
    "def calc_loss(x_target, y_target):\n",
    "    emb = C[x_target].view(-1, EMBEDDING_DIMS * BLOCK_SIZE)\n",
    "    h = emb @ W1 + b1\n",
    "    h = (h - h.mean(dim=0, keepdims=True)) / h.std(dim=0, keepdims=True)\n",
    "    h = torch.tanh(bn_gain * h + bn_bias)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y_target)\n",
    "    return loss\n",
    "\n",
    "calc_loss(x_train, y_train), calc_loss(x_valid, y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "256b36b9-8c2b-4fbf-927e-8f453f69ce2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2226e+00, -5.7514e-01,  1.5726e+00, -8.9563e-01, -4.2299e-01,\n",
      "          3.1737e-01, -3.7347e-01, -4.0377e-01,  2.5963e+00, -2.0262e+00,\n",
      "         -1.2265e+00, -3.2221e-01, -1.0200e+00, -1.4708e-01,  1.2598e-01,\n",
      "         -7.4295e-01,  1.9956e+00, -1.1344e+00,  1.4441e+00,  1.3393e-01,\n",
      "          1.0885e+00,  1.1504e+00, -1.1677e+00, -3.1158e-02,  2.1367e-01,\n",
      "          4.0217e-01, -5.6928e-01,  1.4484e+00,  3.3922e+00,  4.4614e-01,\n",
      "         -2.1129e-01, -3.6620e-01,  1.6677e+00, -3.4504e-01,  1.4493e-01,\n",
      "         -1.5573e+00, -1.1558e+00,  1.1900e+00, -2.9737e-01,  2.2364e+00,\n",
      "         -2.5963e-01,  6.5549e-01, -3.1693e-01,  1.0640e+00, -1.3255e-01,\n",
      "          9.3955e-01,  9.3410e-01, -1.8707e-01,  1.0591e+00,  1.3200e+00,\n",
      "          9.8426e-01,  2.8303e-01, -5.5124e-01, -1.3585e+00, -3.2119e-01,\n",
      "          1.0012e+00,  2.1714e+00, -1.2558e+00, -4.9660e-01,  1.2911e+00,\n",
      "         -9.8416e-01,  2.7136e-01,  8.9102e-01, -3.2857e-01, -1.7429e+00,\n",
      "          3.7507e-01,  1.4067e+00, -4.8757e-01, -5.6440e+00, -5.8432e-03,\n",
      "         -6.0355e-01,  1.2116e+00,  7.0865e-01, -6.0193e-02,  1.3127e+00,\n",
      "          2.2582e+00, -2.2981e-01, -2.1543e+00,  4.2227e-01, -1.2320e+00,\n",
      "          9.8941e-01, -9.9310e-01, -2.2158e+00,  1.1752e+00,  6.2554e-01,\n",
      "         -6.3241e-01,  4.1049e-01,  2.0069e+00,  4.2994e-01, -1.8657e+00,\n",
      "         -5.8367e-01,  1.6590e+00, -1.3689e+00, -9.2733e-01, -1.4679e+00,\n",
      "          1.5406e+00,  3.6552e-01,  7.2312e-01, -6.5126e-01, -1.4946e+00,\n",
      "         -6.1321e-01,  4.8703e-01,  5.9171e-02, -1.0444e+00,  1.0530e+00,\n",
      "         -6.1566e-01, -1.3158e+00,  1.6757e+00,  5.1759e-01, -4.5816e-02,\n",
      "          5.5342e-01,  1.2537e+00,  6.3212e-02, -4.5502e-03, -4.9963e-01,\n",
      "         -1.1529e+00,  3.9663e-01, -2.2010e-02,  1.8032e+00, -1.2527e+00,\n",
      "         -7.0450e-01,  2.7787e+00, -2.4026e-01, -5.8125e-01, -1.0642e+00,\n",
      "         -6.1315e-01,  4.0742e-01,  1.9334e-01, -8.1299e-01, -6.3979e-01,\n",
      "          4.6018e-02,  7.1723e-01, -1.5619e+00,  1.4110e+00, -2.1905e+00,\n",
      "          7.5396e-01,  7.6958e-01, -1.2045e+00,  1.5100e-01,  7.9443e-01,\n",
      "         -1.9178e+00, -1.3618e+00,  4.4364e+00,  4.4075e-01,  1.5033e+00,\n",
      "         -1.3686e+00,  2.5958e+00,  6.2011e-02,  3.7131e-01, -8.8810e-01,\n",
      "         -6.1342e-01,  1.1133e+00, -2.0263e+00,  9.0800e-01, -2.9489e+00,\n",
      "          1.0656e+00,  3.9315e-01,  2.7879e-01, -4.9895e-01, -4.5873e-01,\n",
      "          8.9550e-01, -5.4141e-01, -1.8094e-01, -1.9616e+00, -1.2017e+00,\n",
      "          7.1557e-01,  2.2373e-01,  8.4534e-01, -6.6216e-01,  4.1454e-01,\n",
      "          2.1512e+00,  4.8471e-01,  1.5406e+00, -2.7449e-01, -4.0943e-01,\n",
      "         -1.1631e+00,  6.4856e-01, -2.6859e+00,  1.7720e-01, -6.0795e-01,\n",
      "         -1.4993e-01,  2.1473e-01, -6.9554e-02, -1.2743e+00, -7.4215e-01,\n",
      "          1.3144e+00,  3.4644e-01,  7.7458e-02,  1.0183e+00,  7.7329e-01,\n",
      "         -2.6168e+00,  1.3604e+00, -2.3357e+00, -1.0123e+00,  1.2380e+00,\n",
      "          1.7654e-01,  1.5716e+00, -6.3786e-01, -4.5766e-02, -2.5549e-01]])\n",
      "tensor([[nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "         nan, nan, nan, nan, nan, nan, nan, nan]])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "probability tensor contains either `inf`, `nan` or element < 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [94], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([char_lookup[t] \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens][BLOCK_SIZE:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m---> 21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/opt/miniconda3/envs/practical/lib/python3.10/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn [94], line 14\u001b[0m, in \u001b[0;36msample\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m logits \u001b[38;5;241m=\u001b[39m h \u001b[38;5;241m@\u001b[39m W2 \u001b[38;5;241m+\u001b[39m b2\n\u001b[1;32m     13\u001b[0m probs \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 14\u001b[0m ix \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     15\u001b[0m tokens\u001b[38;5;241m.\u001b[39mappend(ix)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ix \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: probability tensor contains either `inf`, `nan` or element < 0"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "@torch.no_grad()\n",
    "def sample():\n",
    "    tokens = [0] * BLOCK_SIZE\n",
    "    while True:\n",
    "        emb = C[torch.tensor(tokens[-BLOCK_SIZE:])].view(-1, EMBEDDING_DIMS * BLOCK_SIZE)\n",
    "        h = emb @ W1 + b1\n",
    "        print(h)\n",
    "        h = (h - h.mean(dim=0, keepdims=True)) / h.std(dim=0, keepdims=True)\n",
    "        print(h)\n",
    "        h = torch.tanh(bn_gain * h + bn_bias)\n",
    "        logits = h @ W2 + b2\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        ix = torch.multinomial(probs, num_samples=1).item()\n",
    "        tokens.append(ix)\n",
    "        if ix == 0:\n",
    "            break\n",
    "    return \"\".join([char_lookup[t] for t in tokens][BLOCK_SIZE:-1])\n",
    "\n",
    "for i in range(20):\n",
    "    print(sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55cbfb6d-9e9a-469d-bcce-816af65a6a3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.0000e+00, 3.7835e-44, 5.5211e-42, 3.7835e-44]), tensor(95.0000))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = -torch.tensor(1/27.).log()\n",
    "logits = torch.tensor([100.,0.,5.,0.])\n",
    "probs = torch.softmax(logits, dim=0)\n",
    "probs, -probs[2].log()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
