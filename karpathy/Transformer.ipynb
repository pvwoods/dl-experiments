{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "4e74cf4e-ed9c-4321-8074-60403953d4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "371a08fd-0051-4fdf-ba50-d185419a3f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    \n",
    "    def __init__(self, dims:int, heads:int, dropout:float=0.1):\n",
    "        \n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        \n",
    "        # number of dims must be evenly divisible by num heads\n",
    "        assert dims % heads == 0\n",
    "        \n",
    "        self.dims = dims\n",
    "        self.heads = heads\n",
    "        self.head_dims = dims // heads\n",
    "        self.dk_sqrt = math.sqrt(self.head_dims)\n",
    "        \n",
    "        self.qkv_projection = nn.Linear(self.dims,self.dims * 3)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        self.out_projection = nn.Linear(self.dims,self.dims)\n",
    "        self.resid_dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def calculate_attention(self, q, k, v):\n",
    "        \n",
    "        # get similarity of key and query via dot product\n",
    "        qk_similarity = q @ k.transpose(-2, -1)\n",
    "        # normalize values by head dims\n",
    "        normalized_qk_similarity = qk_similarity / self.dk_sqrt\n",
    "        attention = torch.softmax(normalized_qk_similarity, dim=-1)\n",
    "        attention = self.attn_dropout(attention)\n",
    "        out = attention @ v\n",
    "        \n",
    "        return out, attention\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        B, T, C = x.shape # batch size, num tokens, token embedding size\n",
    "        \n",
    "        # linear projection for q, k, v\n",
    "        qkv = self.qkv_projection(x).split(self.dims, dim=2)\n",
    "        \n",
    "        # reshape to (batch, heads, tokens, head_dims)\n",
    "        q, k, v = [cv.view((B, -1, self.heads, self.head_dims)).transpose(1, 2) for cv in qkv]\n",
    "        \n",
    "        # calculatue and apply attention\n",
    "        y, attention = self.calculate_attention(q, k, v)\n",
    "        \n",
    "        # concat all heads\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C)\n",
    "        \n",
    "        # outward projection and residual dropout\n",
    "        y = self.resid_dropout(self.out_projection(y))\n",
    "        \n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "f03c7d91-af36-4bd4-9d7e-9ce633f510fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 12, 100])"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MultiHeadedAttention(100, 20)\n",
    "x = torch.randn(3, 12, 100)\n",
    "model(model(x)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "e8e6bd50-fa53-4c31-a2e6-ec0a3aa265a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dims:int, max_sequence:int):\n",
    "        \n",
    "        super(PositionalEncoder, self).__init__()\n",
    "        \n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.max_sequence = max_sequence\n",
    "        \n",
    "        positional_embedding = torch.zeros(max_sequence, embedding_dims)\n",
    "        div_pos = torch.arange(0, self.embedding_dims, 2)\n",
    "        divisors = torch.exp(div_pos * -(math.log(10000.0) / self.embedding_dims))\n",
    "        positions = torch.arange(max_sequence)[:,None,...]\n",
    "        \n",
    "        positional_embedding[:, 0::2] = torch.sin(positions * divisors)\n",
    "        positional_embedding[:, 1::2] = torch.cos(positions * divisors)\n",
    "        positional_embedding = positional_embedding[None,...]\n",
    "        \n",
    "        self.register_buffer('positional_embedding', positional_embedding)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = Variable(x + self.positional_embedding[:, :x.shape[1]], requires_grad=False)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "e92f8732-33d6-46d8-bcc7-63d6a66a06a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 10, 10])"
      ]
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    \n",
    "    def __init__(self, in_dims:int, hidden_dims:int, dropout:float=0.1):\n",
    "        \n",
    "        super(PositionWiseFFN, self).__init__()\n",
    "        \n",
    "        self.sequence = nn.Sequential(\n",
    "            nn.Linear(in_dims, hidden_dims),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dims, in_dims),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.sequence(x)\n",
    "    \n",
    "pwffn = PositionWiseFFN(10, 100)\n",
    "pwffn(torch.randn(5,10,10)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "a6576369-b47f-4fda-b910-547ca69b483e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 12, 64])"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 att_heads:int = 8, \n",
    "                 att_dims:int = 64, \n",
    "                 ff_dims:int = 256\n",
    "                ):\n",
    "        \n",
    "        super(TransformerBlock, self).__init__()\n",
    "        \n",
    "        self.att_heads = att_heads\n",
    "        self.att_dims = att_dims\n",
    "        self.ff_dims = ff_dims\n",
    "        \n",
    "        self.ln_1 = nn.LayerNorm(self.att_dims)\n",
    "        self.attention = MultiHeadedAttention(self.att_dims, self.att_heads)\n",
    "        self.ln_2 = nn.LayerNorm(self.att_dims)\n",
    "        self.feed_forward = PositionWiseFFN(self.att_dims, self.ff_dims)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = x + self.attention(self.ln_1(x))\n",
    "        x = x + self.feed_forward(self.ln_2(x))\n",
    "        \n",
    "        return x\n",
    "        \n",
    "tb = TransformerBlock()\n",
    "x = torch.randn((3,12,64))\n",
    "tb(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "5495c2e9-f709-4e37-86cc-10dc18d51dd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.3223e-01,  7.2902e-01,  2.3409e-01,  ...,  2.9002e-01,\n",
       "           2.9854e-02, -4.8219e-01],\n",
       "         [-3.6588e-01,  1.2729e+00, -6.3550e-01,  ..., -4.5261e-01,\n",
       "           4.2971e-01, -1.0852e+00],\n",
       "         [-1.3221e+00,  1.4389e-01,  1.1058e+00,  ...,  2.8478e-01,\n",
       "          -4.8847e-02, -5.2399e-01],\n",
       "         ...,\n",
       "         [-9.9639e-01, -5.1359e-02,  1.2110e+00,  ..., -3.3482e-01,\n",
       "           6.1723e-01, -3.4147e-02],\n",
       "         [-7.9969e-01,  8.3777e-01, -9.3797e-02,  ..., -1.2536e+00,\n",
       "           4.5533e-02, -1.2406e+00],\n",
       "         [-1.0880e+00,  3.3933e-01, -3.8144e-01,  ..., -9.4022e-01,\n",
       "           1.1138e-01,  5.3261e-01]],\n",
       "\n",
       "        [[-5.2703e-01, -1.0743e-01, -5.4753e-01,  ...,  2.2119e-01,\n",
       "           5.0657e-01,  3.0102e-01],\n",
       "         [-2.4397e-01,  7.3370e-01,  1.6236e-01,  ...,  2.4425e-01,\n",
       "           5.1760e-02, -3.4514e-01],\n",
       "         [-4.7319e-01,  1.3166e+00,  5.3485e-02,  ...,  4.0152e-01,\n",
       "          -1.4632e-01, -7.4426e-01],\n",
       "         ...,\n",
       "         [-2.9669e-01,  1.2380e+00,  2.4786e-01,  ..., -4.7544e-01,\n",
       "           3.7622e-01,  5.1451e-01],\n",
       "         [ 4.8332e-02,  1.0832e+00, -2.1583e-02,  ..., -5.3409e-01,\n",
       "           6.7368e-01,  6.1331e-01],\n",
       "         [ 8.0336e-01, -9.7814e-02,  8.4783e-01,  ..., -4.9372e-01,\n",
       "          -7.8533e-01,  5.5566e-01]],\n",
       "\n",
       "        [[ 2.3671e-01, -3.5722e-01, -4.0303e-01,  ..., -4.1028e-01,\n",
       "           8.4611e-01,  9.2308e-01],\n",
       "         [ 7.8387e-01, -2.6989e-01, -7.8478e-01,  ...,  1.9295e-01,\n",
       "           5.8868e-01,  2.9634e-01],\n",
       "         [-2.3945e-02,  3.0392e-02, -6.9012e-01,  ...,  4.7146e-01,\n",
       "           1.3360e+00,  1.7164e-01],\n",
       "         ...,\n",
       "         [-2.5243e-02,  2.4666e-01,  1.7853e-01,  ..., -9.1488e-01,\n",
       "          -4.4228e-01,  7.7324e-01],\n",
       "         [-6.6631e-01,  1.0133e+00,  7.7802e-01,  ..., -4.3611e-01,\n",
       "           2.0300e-01,  1.7251e-01],\n",
       "         [ 6.1395e-02,  1.5987e-01, -2.4683e-01,  ..., -1.9766e-02,\n",
       "           1.2861e+00, -6.3275e-01]],\n",
       "\n",
       "        [[ 4.3804e-01, -8.5629e-01, -4.4291e-01,  ...,  6.8313e-01,\n",
       "           1.6290e-01, -2.0506e-01],\n",
       "         [-1.1207e-01, -2.3884e-01,  1.6093e-01,  ..., -1.7567e-01,\n",
       "          -3.6005e-01,  1.0373e+00],\n",
       "         [ 1.4184e-01, -4.4411e-01,  1.1883e+00,  ...,  2.8442e-01,\n",
       "          -2.7126e-01,  9.3836e-01],\n",
       "         ...,\n",
       "         [-4.2001e-01,  9.5357e-01, -3.0892e-01,  ..., -1.2675e+00,\n",
       "          -3.2368e-01, -5.6723e-01],\n",
       "         [ 4.5192e-04,  6.5459e-01,  1.6161e-01,  ..., -4.7048e-01,\n",
       "           6.6839e-01, -1.6504e-01],\n",
       "         [-9.5457e-01,  4.7935e-02, -1.0180e+00,  ..., -9.8465e-01,\n",
       "           1.6372e-01, -5.1989e-01]]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DecoderTransformer(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 vocab_size:int = 32,\n",
    "                 max_sequence:int = 64,\n",
    "                 embedding_dims:int = 64,\n",
    "                 att_heads:int = 8,\n",
    "                 ff_dims:int = 256,\n",
    "                 dropout:float = 0.1\n",
    "                ):\n",
    "        \n",
    "        super(DecoderTransformer, self).__init__()\n",
    "        \n",
    "        self.vocab_size = vocab_size\n",
    "        self.max_sequence = max_sequence\n",
    "        self.embedding_dims = embedding_dims\n",
    "        self.att_heads = att_heads\n",
    "        self.ff_dims = ff_dims\n",
    "        \n",
    "        self.sequence = nn.Sequential(\n",
    "            nn.Embedding(self.vocab_size, self.embedding_dims),\n",
    "            PositionalEncoder(self.embedding_dims, self.max_sequence), # maybe just change this to an embedding too\n",
    "            nn.Dropout(dropout),\n",
    "            TransformerBlock(self.att_heads, self.embedding_dims, self.ff_dims),\n",
    "            nn.LayerNorm(self.embedding_dims)\n",
    "        )\n",
    "        \n",
    "        self.lm_head = nn.Linear(self.embedding_dims, self.vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.sequence(x)\n",
    "        return self.lm_head(x)\n",
    "\n",
    "model = DecoderTransformer()\n",
    "x = torch.randint(31, (4,16))\n",
    "model(x).shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
